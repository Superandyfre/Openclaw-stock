"""
é‡è¦äººç‰©RSSè®¢é˜…ç›‘æ§å™¨

è®¢é˜…åŠ å¯†è´§å¸é¢†åŸŸé‡è¦äººç‰©çš„åšå®¢ã€æ–‡ç« å’Œå®˜æ–¹å£°æ˜

å®Œå…¨å…è´¹ï¼Œæ— éœ€APIå¯†é’¥
"""
import asyncio
import re
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from loguru import logger
import requests
from xml.etree import ElementTree as ET

try:
    import feedparser
    FEEDPARSER_AVAILABLE = True
except ImportError:
    FEEDPARSER_AVAILABLE = False
    logger.warning("feedparseræœªå®‰è£…ï¼ŒRSSç›‘æ§å°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")


class InfluencerRSSMonitor:
    """
    é‡è¦äººç‰©RSSè®¢é˜…ç›‘æ§å™¨
    
    ç›‘æ§åŠ å¯†è´§å¸é¢†åŸŸé‡è¦äººç‰©çš„åšå®¢å’Œæ–‡ç« 
    å®Œå…¨å…è´¹ï¼Œæ— éœ€APIå¯†é’¥
    """
    
    # é¢„è®¾çš„é‡è¦äººç‰©RSSè®¢é˜…æº
    IMPORTANT_FEEDS = {
        'vitalik_buterin': {
            'name': 'Vitalik Buterin',
            'role': 'Ethereumåˆ›å§‹äºº',
            'rss_url': 'https://vitalik.eth.limo/feed.xml',
            'website': 'https://vitalik.eth.limo',
            'keywords': ['Ethereum', 'blockchain', 'scaling', 'Layer2', 'PoS'],
            'importance': 'VERY_HIGH'
        },
        'michael_saylor': {
            'name': 'Michael Saylor',
            'role': 'MicroStrategy CEO',
            'rss_url': None,  # éœ€è¦é€šè¿‡Twitteræˆ–Mediumè·å–
            'website': 'https://michael.com',
            'keywords': ['Bitcoin', 'MicroStrategy', 'digital asset', 'store of value'],
            'importance': 'HIGH'
        },
        'ark_invest': {
            'name': 'ARK Invest (Cathie Wood)',
            'role': 'æŠ•èµ„æœºæ„',
            'rss_url': 'https://ark-invest.com/articles/feed/',
            'website': 'https://ark-invest.com',
            'keywords': ['Bitcoin', 'innovation', 'disruptive', 'technology'],
            'importance': 'HIGH'
        },
        'coindesk': {
            'name': 'CoinDesk',
            'role': 'åŠ å¯†æ–°é—»åª’ä½“',
            'rss_url': 'https://www.coindesk.com/arc/outboundfeeds/rss/',
            'website': 'https://www.coindesk.com',
            'keywords': ['Bitcoin', 'Ethereum', 'crypto', 'regulation', 'market'],
            'importance': 'MEDIUM'
        },
        'cointelegraph': {
            'name': 'Cointelegraph',
            'role': 'åŠ å¯†æ–°é—»åª’ä½“',
            'rss_url': 'https://cointelegraph.com/rss',
            'website': 'https://cointelegraph.com',
            'keywords': ['cryptocurrency', 'blockchain', 'Bitcoin', 'Ethereum'],
            'importance': 'MEDIUM'
        },
        'bitcoin_magazine': {
            'name': 'Bitcoin Magazine',
            'role': 'Bitcoinæ–°é—»åª’ä½“',
            'rss_url': 'https://bitcoinmagazine.com/.rss/full/',
            'website': 'https://bitcoinmagazine.com',
            'keywords': ['Bitcoin', 'Lightning Network', 'mining', 'adoption'],
            'importance': 'MEDIUM'
        },
        'ethereum_foundation': {
            'name': 'Ethereum Foundation',
            'role': 'Ethereumå®˜æ–¹',
            'rss_url': 'https://blog.ethereum.org/feed.xml',
            'website': 'https://blog.ethereum.org',
            'keywords': ['Ethereum', 'EIP', 'upgrade', 'research', 'development'],
            'importance': 'HIGH'
        },
        'a16z_crypto': {
            'name': 'a16z Crypto',
            'role': 'æŠ•èµ„æœºæ„',
            'rss_url': 'https://a16zcrypto.com/feed/',
            'website': 'https://a16zcrypto.com',
            'keywords': ['crypto', 'web3', 'investment', 'regulation', 'innovation'],
            'importance': 'HIGH'
        }
    }
    
    def __init__(
        self,
        feeds: Optional[List[str]] = None,
        update_interval_minutes: int = 60
    ):
        """
        åˆå§‹åŒ–RSSç›‘æ§å™¨
        
        Args:
            feeds: è¦ç›‘æ§çš„è®¢é˜…æºåˆ—è¡¨ï¼ˆkeyï¼‰
            update_interval_minutes: æ›´æ–°é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        """
        # ç›‘æ§çš„è®¢é˜…æºï¼ˆé»˜è®¤ç›‘æ§æ‰€æœ‰ï¼‰
        if feeds:
            self.feeds = feeds
        else:
            self.feeds = list(self.IMPORTANT_FEEDS.keys())
        
        self.update_interval = update_interval_minutes
        
        # æ•°æ®ç¼“å­˜
        self.article_cache: Dict[str, List[Dict[str, Any]]] = {}
        self.last_update_time: Dict[str, datetime] = {}
        
        logger.info(f"âœ… InfluencerRSSMonitor åˆå§‹åŒ–")
        logger.info(f"   ç›‘æ§è®¢é˜…æº: {len(self.feeds)}ä¸ª")
    
    def fetch_feed(
        self,
        feed_key: str,
        since_hours: int = 24
    ) -> List[Dict[str, Any]]:
        """
        è·å–RSSè®¢é˜…æºå†…å®¹
        
        Args:
            feed_key: è®¢é˜…æºkey
            since_hours: è·å–å¤šå°‘å°æ—¶å†…çš„æ–‡ç« 
        
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        feed_info = self.IMPORTANT_FEEDS.get(feed_key)
        if not feed_info:
            logger.error(f"æœªçŸ¥çš„è®¢é˜…æº: {feed_key}")
            return []
        
        rss_url = feed_info.get('rss_url')
        if not rss_url:
            logger.warning(f"{feed_info['name']} æš‚æ— RSSè®¢é˜…æº")
            return self._generate_mock_articles(feed_key, 5)
        
        # å¦‚æœæ²¡æœ‰feedparserï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not FEEDPARSER_AVAILABLE:
            return self._generate_mock_articles(feed_key, 5)
        
        try:
            # è·å–RSSå†…å®¹
            feed = feedparser.parse(rss_url)
            
            articles = []
            cutoff_time = datetime.now() - timedelta(hours=since_hours)
            
            for entry in feed.entries[:20]:  # æœ€å¤šè·å–20ç¯‡
                # è§£æå‘å¸ƒæ—¶é—´
                published_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published_time = datetime(*entry.updated_parsed[:6])
                
                # åªè·å–æŒ‡å®šæ—¶é—´å†…çš„æ–‡ç« 
                if published_time and published_time < cutoff_time:
                    continue
                
                # æå–å†…å®¹æ‘˜è¦
                summary = ''
                if hasattr(entry, 'summary'):
                    summary = entry.summary[:500]
                elif hasattr(entry, 'description'):
                    summary = entry.description[:500]
                
                # æ¸…ç†HTMLæ ‡ç­¾
                summary = re.sub(r'<[^>]+>', '', summary)
                
                articles.append({
                    'feed': feed_key,
                    'author': feed_info['name'],
                    'title': entry.title if hasattr(entry, 'title') else 'No title',
                    'summary': summary,
                    'link': entry.link if hasattr(entry, 'link') else '',
                    'published': published_time.isoformat() if published_time else datetime.now().isoformat(),
                    'timestamp': datetime.now().isoformat()
                })
            
            logger.info(f"è·å– {feed_info['name']} çš„ {len(articles)} ç¯‡æ–‡ç« ")
            return articles
        
        except Exception as e:
            logger.error(f"è·å–RSSè®¢é˜…å¤±è´¥ {feed_info['name']}: {e}")
            return self._generate_mock_articles(feed_key, 5)
    
    def analyze_articles(
        self,
        articles: List[Dict[str, Any]],
        feed_key: str
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–‡ç« å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            feed_key: è®¢é˜…æºkey
        
        Returns:
            åˆ†æç»“æœ
        """
        if not articles:
            return {'error': 'æ— æ–‡ç« æ•°æ®'}
        
        feed_info = self.IMPORTANT_FEEDS.get(feed_key, {})
        keywords = feed_info.get('keywords', [])
        
        # 1. å…³é”®è¯ç»Ÿè®¡
        keyword_counts = {kw: 0 for kw in keywords}
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            combined_text = title + ' ' + summary
            
            for kw in keywords:
                if kw.lower() in combined_text:
                    keyword_counts[kw] += 1
        
        # 2. æƒ…ç»ªåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰
        bullish_keywords = [
            'bullish', 'positive', 'growth', 'adoption', 'breakthrough', 'innovation',
            'surge', 'rally', 'optimistic', 'upgrade', 'improvement', 'success'
        ]
        bearish_keywords = [
            'bearish', 'negative', 'crisis', 'regulation', 'ban', 'crackdown',
            'drop', 'crash', 'concern', 'risk', 'warning', 'decline'
        ]
        
        bullish_count = 0
        bearish_count = 0
        
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            combined = title + ' ' + summary
            
            bullish_count += sum(1 for kw in bullish_keywords if kw in combined)
            bearish_count += sum(1 for kw in bearish_keywords if kw in combined)
        
        total_sentiment_signals = bullish_count + bearish_count
        if total_sentiment_signals > 0:
            sentiment_score = (bullish_count - bearish_count) / total_sentiment_signals
        else:
            sentiment_score = 0
        
        # æƒ…ç»ªåˆ†ç±»
        if sentiment_score > 0.3:
            sentiment_label = 'POSITIVE'
        elif sentiment_score > 0.1:
            sentiment_label = 'SLIGHTLY_POSITIVE'
        elif sentiment_score < -0.3:
            sentiment_label = 'NEGATIVE'
        elif sentiment_score < -0.1:
            sentiment_label = 'SLIGHTLY_NEGATIVE'
        else:
            sentiment_label = 'NEUTRAL'
        
        # 3. ä¸»é¢˜åˆ†ç±»
        topics = {
            'Bitcoin': 0,
            'Ethereum': 0,
            'DeFi': 0,
            'NFT': 0,
            'Regulation': 0,
            'Technology': 0,
            'Market': 0
        }
        
        topic_keywords = {
            'Bitcoin': ['bitcoin', 'btc', 'satoshi'],
            'Ethereum': ['ethereum', 'eth', 'vitalik'],
            'DeFi': ['defi', 'decentralized finance', 'liquidity', 'yield'],
            'NFT': ['nft', 'non-fungible', 'collectible', 'metaverse'],
            'Regulation': ['regulation', 'sec', 'government', 'policy', 'law'],
            'Technology': ['technology', 'protocol', 'upgrade', 'development', 'scalability'],
            'Market': ['market', 'price', 'trading', 'investment', 'valuation']
        }
        
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            combined = title + ' ' + summary
            
            for topic, kws in topic_keywords.items():
                if any(kw in combined for kw in kws):
                    topics[topic] += 1
        
        # 4. é‡è¦æ€§è¯„åˆ†ï¼ˆåŸºäºä½œè€…æƒé‡ï¼‰
        importance = feed_info.get('importance', 'MEDIUM')
        importance_score = {
            'VERY_HIGH': 1.0,
            'HIGH': 0.8,
            'MEDIUM': 0.6,
            'LOW': 0.4
        }.get(importance, 0.6)
        
        analysis = {
            'feed': feed_key,
            'author': feed_info.get('name', feed_key),
            'role': feed_info.get('role', 'Unknown'),
            'importance': importance,
            'importance_score': importance_score,
            'timestamp': datetime.now().isoformat(),
            'article_count': len(articles),
            'keyword_mentions': keyword_counts,
            'sentiment': {
                'score': sentiment_score,
                'label': sentiment_label,
                'positive_signals': bullish_count,
                'negative_signals': bearish_count
            },
            'topics': topics,
            'recent_articles': [
                {
                    'title': article['title'],
                    'summary': article['summary'][:150] + '...' if len(article['summary']) > 150 else article['summary'],
                    'link': article['link'],
                    'published': article['published']
                }
                for article in articles[:5]  # æœ€è¿‘5ç¯‡
            ]
        }
        
        # ç¼“å­˜ç»“æœ
        if feed_key not in self.article_cache:
            self.article_cache[feed_key] = []
        
        self.article_cache[feed_key].extend(articles)
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.article_cache[feed_key]) > 100:
            self.article_cache[feed_key] = self.article_cache[feed_key][-100:]
        
        self.last_update_time[feed_key] = datetime.now()
        
        return analysis
    
    def monitor_all_feeds(
        self,
        since_hours: int = 24
    ) -> List[Dict[str, Any]]:
        """
        ç›‘æ§æ‰€æœ‰é…ç½®çš„è®¢é˜…æº
        
        Args:
            since_hours: è·å–å¤šå°‘å°æ—¶å†…çš„æ–‡ç« 
        
        Returns:
            æ‰€æœ‰è®¢é˜…æºçš„åˆ†æç»“æœ
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ”„ å¼€å§‹RSSè®¢é˜…ç›‘æ§ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"{'='*70}")
        
        results = []
        
        for feed_key in self.feeds:
            try:
                # è·å–æ–‡ç« 
                articles = self.fetch_feed(feed_key, since_hours=since_hours)
                
                # åˆ†ææ–‡ç« 
                if articles:
                    analysis = self.analyze_articles(articles, feed_key)
                    results.append(analysis)
                    
                    # æ‰“å°æ‘˜è¦
                    sentiment = analysis.get('sentiment', {})
                    logger.info(f"ğŸ“° {analysis['author']}: {analysis['article_count']}ç¯‡æ–‡ç« , "
                              f"æƒ…ç»ª={sentiment['label']}, "
                              f"é‡è¦æ€§={analysis['importance']}")
                
                # é¿å…é€Ÿç‡é™åˆ¶
                import time
                time.sleep(1)
            
            except Exception as e:
                logger.error(f"ç›‘æ§è®¢é˜…æºå¤±è´¥ {feed_key}: {e}")
        
        logger.info(f"{'='*70}")
        logger.info(f"âœ… RSSç›‘æ§å®Œæˆï¼Œå…±åˆ†æ {len(results)} ä¸ªè®¢é˜…æº")
        logger.info(f"{'='*70}\n")
        
        return results
    
    def _generate_mock_articles(self, feed_key: str, count: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ–‡ç« ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        import random
        
        feed_info = self.IMPORTANT_FEEDS.get(feed_key, {})
        author = feed_info.get('name', 'Unknown Author')
        
        mock_titles = [
            "The Future of Ethereum: Scaling Solutions and Layer 2 Networks",
            "Bitcoin's Role as Digital Gold in the Modern Economy",
            "Understanding DeFi: Opportunities and Risks",
            "Regulatory Landscape for Cryptocurrencies in 2026",
            "Innovation in Blockchain Technology: What's Next?",
            "MicroStrategy's Bitcoin Strategy: A Deep Dive",
            "Ethereum Merge Anniversary: One Year Later",
            "The Rise of Institutional Crypto Adoption"
        ]
        
        articles = []
        base_time = datetime.now()
        
        for i in range(count):
            articles.append({
                'feed': feed_key,
                'author': author,
                'title': random.choice(mock_titles),
                'summary': 'This is a mock article summary for testing purposes. The full content would contain detailed analysis and insights about cryptocurrency markets and technology.',
                'link': f'https://example.com/article/{i}',
                'published': (base_time - timedelta(hours=random.randint(1, 23))).isoformat(),
                'timestamp': datetime.now().isoformat()
            })
        
        return articles
    
    def get_summary_report(self, analyses: List[Dict[str, Any]]) -> str:
        """
        ç”ŸæˆRSSç›‘æ§æ‘˜è¦æŠ¥å‘Š
        
        Args:
            analyses: è®¢é˜…æºåˆ†æç»“æœåˆ—è¡¨
        
        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        if not analyses:
            return "æ— RSSè®¢é˜…æ•°æ®"
        
        report = []
        report.append("\n" + "="*70)
        report.append("ğŸ“š é‡è¦äººç‰©RSSè®¢é˜…æŠ¥å‘Š")
        report.append("="*70)
        report.append(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ç›‘æ§è®¢é˜…æº: {len(analyses)}ä¸ª")
        report.append("")
        
        # æ•´ä½“æƒ…ç»ªç»Ÿè®¡
        total_positive = sum(1 for a in analyses if a.get('sentiment', {}).get('label') in ['POSITIVE', 'SLIGHTLY_POSITIVE'])
        total_negative = sum(1 for a in analyses if a.get('sentiment', {}).get('label') in ['NEGATIVE', 'SLIGHTLY_NEGATIVE'])
        total_neutral = len(analyses) - total_positive - total_negative
        
        report.append("ã€æ•´ä½“æƒ…ç»ªã€‘")
        report.append(f"  æ­£é¢: {total_positive}ä¸ªè®¢é˜…æº")
        report.append(f"  è´Ÿé¢: {total_negative}ä¸ªè®¢é˜…æº")
        report.append(f"  ä¸­æ€§: {total_neutral}ä¸ªè®¢é˜…æº")
        report.append("")
        
        # çƒ­é—¨ä¸»é¢˜
        all_topics = {}
        for analysis in analyses:
            for topic, count in analysis.get('topics', {}).items():
                if topic not in all_topics:
                    all_topics[topic] = 0
                all_topics[topic] += count
        
        if all_topics:
            report.append("ã€çƒ­é—¨ä¸»é¢˜ã€‘")
            for topic, count in sorted(all_topics.items(), key=lambda x: x[1], reverse=True):
                if count > 0:
                    report.append(f"  {topic}: {count}ç¯‡æ–‡ç« ")
            report.append("")
        
        # å„è®¢é˜…æºè¯¦æƒ…
        report.append("ã€è®¢é˜…æºè¯¦æƒ…ã€‘")
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_analyses = sorted(analyses, key=lambda x: x.get('importance_score', 0), reverse=True)
        
        for analysis in sorted_analyses:
            author = analysis.get('author', 'Unknown')
            role = analysis.get('role', '')
            article_count = analysis['article_count']
            sentiment = analysis.get('sentiment', {})
            importance = analysis.get('importance', 'MEDIUM')
            
            report.append(f"\n  ğŸ“° {author} ({role})")
            report.append(f"     é‡è¦æ€§: {importance}")
            report.append(f"     æ–‡ç« æ•°: {article_count}")
            report.append(f"     æƒ…ç»ª: {sentiment.get('label', 'UNKNOWN')} (å¾—åˆ†: {sentiment.get('score', 0):.2f})")
            
            # æœ€æ–°æ–‡ç« æ ‡é¢˜
            recent_articles = analysis.get('recent_articles', [])
            if recent_articles:
                report.append(f"     æœ€æ–°æ–‡ç« :")
                for i, article in enumerate(recent_articles[:2], 1):  # åªæ˜¾ç¤ºæœ€æ–°2ç¯‡
                    title = article['title'][:60] + '...' if len(article['title']) > 60 else article['title']
                    report.append(f"       {i}. {title}")
        
        report.append("\n" + "="*70)
        
        return '\n'.join(report)
